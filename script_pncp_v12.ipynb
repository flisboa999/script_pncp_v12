{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaea477",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CÉLULA 1: Carregando todas as bibliotecas... ---\n",
      "Bibliotecas importadas com sucesso.\n"
     ]
    }
   ],
   "source": [
    "# CÉLULA 1: IMPORTS GERAIS E DE ML\n",
    "# Importar TODAS as ferramentas necessárias de uma só vez.\n",
    "     \n",
    "\n",
    "print(\"--- CÉLULA 1: Carregando todas as bibliotecas... ---\")\n",
    "\n",
    "# --- Bibliotecas de Coleta de Dados (ETL da API) ---\n",
    "\n",
    "# 'requests' é a biblioteca que faz as requisições síncronas HTTP para a API do PNCP.\n",
    "import requests\n",
    "\n",
    "# 'sqlite3' biblioteca de banco de dados nativa e leve para operações SQL (os arquivos .db).\n",
    "import sqlite3\n",
    "\n",
    "# 'json' essencial realizar operações na resposta da API, transformar em Dicionário Python, facilitar leitura (dumps)\n",
    "import json\n",
    "\n",
    "# 'time' é usado para o `time.sleep()`, pausa no código pra não floodar a API (rate limiting).\n",
    "import time\n",
    "\n",
    "# 'datetime' é usado p/ obter data e hora atuais, útil para logs e timestamps no banco de dados.\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# --- Bibliotecas de Análise de Dados e ML ---\n",
    "\n",
    "# 'pandas' ('pd') - Ferramenta principal de análise de dados\n",
    "# Carrega, limpa e manipula os dados em tabelas (DataFrames).\n",
    "import pandas as pd\n",
    "\n",
    "# 'numpy' ('np') Biblioteca utilizada para cálculos matemáticos - pandas necessita dela no funcionamento\n",
    "import numpy as np\n",
    "\n",
    "# 're' é a biblioteca de \"Regular Expressions\" (Expressões Regulares)- limpeza e normalização de textos\n",
    "import re\n",
    "\n",
    "# 'joblib' é a ferramenta que usamos para gerar nosso modelo de ML treinado\n",
    "# e salvá-lo em um arquivo (o 'modelo_relevancia.joblib').\n",
    "import joblib \n",
    "\n",
    "# 'nltk' (Natural Language Toolkit) é a principal biblioteca de Processamento de Linguagem Natural.\n",
    "# 'stopwords' é uma lista de palavras comuns ('de', 'a', 'o', 'com') que não têm valor para o modelo.\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# --- Bibliotecas do Scikit-learn (O \"Cérebro\" do ML) ---\n",
    "\n",
    "# 'train_test_split' é a função que divide nosso \"gabarito\" em duas partes:\n",
    "# 1. Dados de Treino (80%): O que o modelo vai \"estudar\".\n",
    "# 2. Dados de Teste (20%): O que o modelo vai \"fazer a prova\" (dados desconhecidos pelo modelo).\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 'TfidfVectorizer' serve basicamente para transformar uma lista de textos em uma \n",
    "# matriz de números de forma inteligente (TF-IDF) dando mais peso para palavras raras e \n",
    "# importantes (ex: \"vazão\" \"medidor\") e menos peso para palavras comuns (ex: \"aquisição\" \"contratação\").\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 'SGDClassifier' (Stochastic Gradient Descent Classifier) é o algoritmo de ML escolhido.\n",
    "# Ele é leve, rápido e muito eficaz para classificação de texto. É o \"aluno\" que vamos treinar.\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# 'Pipeline' é o \"organizador\". Ele nos permite combinar o uso do 'TfidfVectorizer' (tradutor)\n",
    "# e o 'SGDClassifier' (aluno) em uma única execução. Isso garante que qualquer\n",
    "# dado novo (na produção) passe exatamente pelo mesmo processo de tradução do treino.\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# 'classification_report' e 'accuracy_score' são as ferramentas de \"nota\".\n",
    "# Elas geram o \"boletim\" (Relatório de Classificação) que nos diz o quão bem o modelo\n",
    "# se saiu na prova (Precision, Recall, F1-score).\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "print(\"Bibliotecas importadas com sucesso.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37eb5282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CÉLULA 2: Definindo todas as configurações... ---\n",
      "Total de 207 stopwords em português carregadas.\n",
      "Configurações concluídas.\n"
     ]
    }
   ],
   "source": [
    "# CÉLULA 2: CONFIGURAÇÕES GLOBAIS\n",
    "# Define TODOS os nomes de arquivos, parâmetros e segredos em um lugar só.\n",
    "# Mudar qualquer coisa aqui (ex: NOME_DB_PRODUCAO) muda o comportamento do script inteiro.\n",
    "\n",
    "print(\"--- CÉLULA 2: Definindo todas as configurações... ---\")\n",
    "\n",
    "# --- 1. Configs da API e DB de Coleta ---\n",
    "\n",
    "# Endereço base da API\n",
    "BASE_URL = \"https://pncp.gov.br/api/consulta\"\n",
    "\n",
    "# O caminho específico da API que queremos (Contratações com recebimento de proposta em aberto)\n",
    "ENDPOINT = \"/v1/contratacoes/proposta\"\n",
    "\n",
    "# Junção dos dois para formar o URL completo\n",
    "URL_FINAL = BASE_URL + ENDPOINT\n",
    "\n",
    "# Parâmetros de filtro para a coleta de PRODUÇÃO (399 páginas)\n",
    "# Isso é o que dizemos à API: \"Eu quero...\"\n",
    "params_coleta_producao = {\n",
    "    'dataFinal': 20251216,                # \"...licitações até esta data.\"\n",
    "    'codigoModalidadeContratacao': 6,     # \"...apenas Pregão.\" #6 Pregão / 8 #Dispensa\n",
    "    'pagina': 1,                          # \"...começando da página 1.\"\n",
    "    'tamanhoPagina': 50                   # \"...me dê 50 itens por página.\"\n",
    "}\n",
    "\n",
    "# Limite de paginação para ser usado na função\n",
    "LIMITE_PAGINAS_PRODUCAO = 399\n",
    "\n",
    "\n",
    "# Nome do DB onde os dados de PRODUÇÃO (os 19.9k) serão salvos\n",
    "# A Célula 7 (Ação 1) vai escrever neste arquivo.\n",
    "# A Célula 10 (Ação 4) vai ler deste arquivo.\n",
    "NOME_DB_PRODUCAO = \"pncp_data_19000.db\"\n",
    "\n",
    "\n",
    "# --- 2. Configs de TREINAMENTO ---\n",
    "\n",
    "#  DB de 500 amostras pré-rotulado com a maioria dos '0's (irrelevantes)\n",
    "NOME_DB_TREINO_NEGATIVOS = \"pncp_data_500.db\"\n",
    "\n",
    "# Lista com IDX dos '1's (relevantes) que eu anotei manualmente do  DB de 500 de 0s Irrelevantes\n",
    "# Posteriormente o é feito o rotulamento desses dados no Banco\n",
    "IDS_RELEVANTES_CADERNO = [5, 85, 164, 199, 319, 327, 463]\n",
    "\n",
    "# Datasets em arquivos CSV que contêm os '1's (relevantes) puros, rotulados manualmente\n",
    "# dataset_2024 -> 94 relevantes\n",
    "# dataset_2025 -> 105 relevantes\n",
    "ARQUIVOS_CSV_POSITIVOS = [\"dataset_2024.csv\", \"dataset_2025.csv\"]\n",
    "\n",
    "\n",
    "# O \"Gabarito\" final: O arquivo CSV que a Célula 8 (ETL) vai criar - Dataset combinado CSVs 2024 + 2025 + pncp_data_500 \n",
    "NOME_ARQUIVO_GABARITO = \"dataset.csv\"\n",
    "\n",
    "# O \"Cérebro\": O arquivo final que a Célula 9 (Treino) vai criar\n",
    "NOME_ARQUIVO_MODELO = \"modelo_relevancia.joblib\"\n",
    "\n",
    "# --- 3. Configs de PRODUÇÃO (Inferência/Filtro) ---\n",
    "\n",
    "# O banco de dados final que a Célula 10 (Produção) vai criar,\n",
    "# contendo apenas as licitações filtradas \n",
    "NOME_DB_FILTRADO = \"licitacoes_filtradas_19k.db\"\n",
    "\n",
    "# Necessário processar por lote (batch) para não usar toda memória RAM\n",
    "# Se não colocar esse limite trava o Kernel do Jupyter e falha a execução\n",
    "#  Define Quantos itens do DB de 19.9k vamos processar de cada vez.\n",
    "TAMANHO_DO_LOTE = 2000 \n",
    "\n",
    "\n",
    "# --- 4. Configs do NLTK (Stopwords) ---\n",
    "\n",
    "# Tenta carregar a lista de stopwords...\n",
    "try:\n",
    "    lista_stopwords = stopwords.words('portuguese')\n",
    "# ...se falhar (porque nunca foi baixado)...\n",
    "except LookupError:\n",
    "     print(\"ERRO: Stopwords não carregadas. Baixando 'stopwords' do NLTK...\")\n",
    "     # ...baixa o pacote necessário...\n",
    "     nltk.download('stopwords')\n",
    "     # ...e tenta de novo.\n",
    "     lista_stopwords = stopwords.words('portuguese')\n",
    "\n",
    "# Converte a *lista* ['o', 'a', 'de'] em um *set* {'o', 'a', 'de'}.\n",
    "# Isso faz a verificação (palavra in stop_words_pt) ser milhares de vezes mais rápida.\n",
    "stop_words_pt = set(lista_stopwords)\n",
    "print(f\"Total de {len(stop_words_pt)} stopwords em português carregadas.\")\n",
    "\n",
    "print(\"Configurações concluídas.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c2cce036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CÉLULA 3: Função 'configurar_db' definida. ---\n"
     ]
    }
   ],
   "source": [
    "# CÉLULA 3: DEFINIÇÃO DA FERRAMENTA - configurar_db\n",
    "# Define o esquema do nosso banco de dados (table) no no SQLite.\n",
    "\n",
    "# Essa Função só roda quando \"chamar\" configurar_db() no código, por isso é declarada antes\n",
    "def configurar_db(nome_do_banco):\n",
    "    \"\"\"Cria a tabela 'contratacoes' no DB especificado se ela não existir.\"\"\"\n",
    "    \n",
    "    # Bloco try/except para capturar erros de banco de dados\n",
    "    try:\n",
    "        # Conecta ao arquivo .db (ou cria se não existir)\n",
    "        conn = sqlite3.connect(nome_do_banco)\n",
    "        # Cria um \"cursor\", que é o objeto que executa os comandos SQL\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        print(f\"Configurando banco de dados: {nome_do_banco}\")\n",
    "        \n",
    "        # 1. Criar a tabela (só se ela não existir)\n",
    "        # Usamos \"\"\" (aspas triplas) para um comando SQL de múltiplas linhas\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS contratacoes (\n",
    "                id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                pagina_coleta INTEGER,\n",
    "                timestamp_coleta TEXT,\n",
    "                \n",
    "                -- Esta coluna vai guardar o JSON \"cru\", como um backup/data lake\n",
    "                dados_json TEXT, \n",
    "                \n",
    "                -- [JSON1] Colunas \"Geradas\"\n",
    "                -- O próprio SQLite vai abrir o 'dados_json' e extrair esses campos.\n",
    "                -- 'STORED' significa que o dado é salvo em disco, permitindo indexação.\n",
    "                \n",
    "                pncp_id TEXT AS (json_extract(dados_json, '$.numeroControlePNCP')) STORED,\n",
    "                orgao_nome TEXT AS (json_extract(dados_json, '$.orgaoEntidade.razaosocial')) STORED,\n",
    "                uf_sigla TEXT AS (json_extract(dados_json, '$.unidadeOrgao.ufSigla')) STORED,\n",
    "                \n",
    "                -- A coluna mais importante para nós: o texto do objeto\n",
    "                objeto_compra TEXT AS (json_extract(dados_json, '$.objetoCompra')) STORED,\n",
    "                \n",
    "                -- Garante que o mesmo 'pncp_id' não seja inserido duas vezes\n",
    "                UNIQUE(pncp_id) \n",
    "            );\n",
    "        \"\"\")\n",
    "\n",
    "        # 2. Criar IDXs (só se não existirem)\n",
    "        # Um índice torna as buscas (SELECT) por esses campos muito mais rápidas.\n",
    "        cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_pncp_id ON contratacoes (pncp_id);\")\n",
    "        cursor.execute(\"CREATE INDEX IF NOT EXISTS idx_objeto_compra ON contratacoes (objeto_compra);\")\n",
    "        \n",
    "        # Confirma (salva) as mudanças no banco de dados\n",
    "        conn.commit()\n",
    "        # Fecha a conexão com o arquivo\n",
    "        conn.close()\n",
    "        print(f\"Banco '{nome_do_banco}' e tabela 'contratacoes' configurados.\")\n",
    "    \n",
    "    # Se der erro (ex: disco cheio, permissão negada)\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"ERRO ao configurar o DB {nome_do_banco}: {e}\")\n",
    "\n",
    "print(\"--- CÉLULA 3: Função 'configurar_db' definida. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd8d4653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CÉLULA 4: Funções 'fetch_com_retry' e 'inserir_dados' definidas. ---\n"
     ]
    }
   ],
   "source": [
    "# CÉLULA 4: DEFINIÇÃO DAS FERRAMENTAS - fetch_com_retry e inserir_dados\n",
    "# Define a função que chama a API (fetch) e o função que insere os dados no banco.\n",
    "\n",
    "\n",
    "def fetch_com_retry(url, params, max_tentativas=5, backoff_inicial=1):\n",
    "    \"\"\"Busca dados da API com retry e backoff exponencial.\"\"\"\n",
    "    \n",
    "    # Cabeçalho HTTP simples\n",
    "    headers = {'accept': '*/*'}\n",
    "    tentativas = 0\n",
    "    backoff = backoff_inicial\n",
    "    \n",
    "    # Loop de tentativas (vai tentar até 'max_tentativas')\n",
    "    while tentativas < max_tentativas:\n",
    "        try:\n",
    "            # A chamada HTTP real. 'timeout=30' desiste se a API não responder em 30s.\n",
    "            resposta = requests.get(url, params=params, headers=headers, timeout=30)\n",
    "            \n",
    "            # Código 200 = SUCESSO\n",
    "            if resposta.status_code == 200:\n",
    "                # Converte o texto (JSON) da resposta em um Dicionário Python\n",
    "                dados_resposta = resposta.json()\n",
    "                \n",
    "                # Checagem de segurança: a resposta é um dict e tem a chave 'data'?\n",
    "                if isinstance(dados_resposta, dict) and 'data' in dados_resposta:\n",
    "                    \n",
    "                    # Extrai a lista de licitações\n",
    "                    dados_lista = dados_resposta.get('data', [])\n",
    "                    \n",
    "                    # Extrai o número total de páginas (para o log)\n",
    "                    total_paginas = dados_resposta.get('totalPaginas') \n",
    "                    \n",
    "                    # Retorna os dois valores (em uma tupla)\n",
    "                    return (dados_lista, total_paginas)\n",
    "                else:\n",
    "                    # A API respondeu 200, mas o JSON veio zuado.\n",
    "                    return ([], None)\n",
    "            \n",
    "            # Código 204 = Fim dos dados. A API diz \"Ok, mas não tenho mais nada para te dar\".\n",
    "            if resposta.status_code == 204:\n",
    "                return ([], None) # Sinaliza o fim.\n",
    "            \n",
    "            # Qualquer outro erro (ex: 500, 403), imprime e tenta de novo.\n",
    "            print(f\"Erro {resposta.status_code}. Tentando novamente em {backoff}s...\")\n",
    "        \n",
    "        # Se a rede falhar (ex: Wi-Fi caiu)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Erro de conexão: {e}. Tentando novamente em {backoff}s...\")\n",
    "        \n",
    "        # Espera antes de tentar de novo\n",
    "        time.sleep(backoff)\n",
    "        tentativas += 1\n",
    "        backoff *= 2 # \"Backoff exponencial\": espera 1s, 2s, 4s, 8s...\n",
    "        \n",
    "    print(\"Número máximo de tentativas atingido. Falha ao buscar dados.\")\n",
    "    return None # Retorna None em caso de falha total\n",
    "\n",
    "def inserir_dados(nome_db, dados_pagina, pagina_coletada):\n",
    "    \"\"\"Insere uma lista de registros (uma página) no SQLite.\"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(nome_db)\n",
    "        cursor = conn.cursor()\n",
    "        \n",
    "        # Pega a hora atual para registrar quando o dado foi coletado\n",
    "        timestamp_atual = datetime.now().strftime('%Y-%m-%d %H:%M:%S') \n",
    "        \n",
    "        registros_inseridos = 0\n",
    "        \n",
    "        # Loop nos 50 itens da página\n",
    "        for registro in dados_pagina:\n",
    "            \n",
    "            # Converte o dict Python de volta para uma string JSON para salvar no DB\n",
    "            registro_json_str = json.dumps(registro)\n",
    "            \n",
    "            # Insere os dados. 'INSERT OR IGNORE' é o comando que pula duplicatas\n",
    "            # (baseado na 'UNIQUE(pncp_id)' que definimos na Célula 3)\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT OR IGNORE INTO contratacoes (pagina_coleta, timestamp_coleta, dados_json)\n",
    "                VALUES (?, ?, ?);\n",
    "            \"\"\", (pagina_coletada, timestamp_atual, registro_json_str))\n",
    "            \n",
    "            # 'cursor.rowcount' diz se a última linha foi inserida (1) ou ignorada (0)\n",
    "            if cursor.rowcount > 0:\n",
    "                registros_inseridos += 1\n",
    "            \n",
    "        conn.commit() # Salva as 50 inserções de uma vez\n",
    "        print(f\"   -> [DB] {registros_inseridos}/{len(dados_pagina)} registros novos salvos na página {pagina_coletada}.\")\n",
    "    except sqlite3.Error as e:\n",
    "        print(f\"   -> [ERRO DB] Falha ao inserir dados: {e}.\")\n",
    "    finally:\n",
    "        if conn:\n",
    "            conn.close()\n",
    "\n",
    "print(\"--- CÉLULA 4: Funções 'fetch_com_retry' e 'inserir_dados' definidas. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "adad7adf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CÉLULA 5: Função 'buscar_dados_paginados' definida. ---\n"
     ]
    }
   ],
   "source": [
    "# CÉLULA 5: DEFINIÇÃO DA FERRAMENTA - buscar_dados_paginados\n",
    "# Defineq a função que gerencia a coleta (fetch_com_retry) e inserção no db (inserir_dados).\n",
    "\n",
    "# Esta função orquestra todo o processo de coleta\n",
    "def buscar_dados_paginados(nome_do_banco, url_completa, parametros, limite_pagina=None):\n",
    "    \"\"\"Busca todos os dados da API e salva no DB especificado.\"\"\"\n",
    "    \n",
    "    # 1. Garante que o DB e a tabela existam ANTES de começar.\n",
    "    #    (Chama a função da Célula 3)\n",
    "    configurar_db(nome_do_banco)\n",
    "\n",
    "    # Inicia o contador de páginas\n",
    "    pagina_atual = 1\n",
    "    total_registros_processados = 0\n",
    "    total_paginas_api = '???' # Começa sem saber o total\n",
    "    \n",
    "    print(\"--- INICIANDO BUSCA PAGINADA ---\")\n",
    "    if limite_pagina:\n",
    "        print(f\"--- Modo LIMITE DE PÁGINAS Ativado: Parar após página {limite_pagina} ---\")\n",
    "    \n",
    "    # Loop infinito que só para com 'break'\n",
    "    while True:\n",
    "        \n",
    "        # Checagem de segurança (limite de páginas)\n",
    "        if limite_pagina is not None and pagina_atual > limite_pagina:\n",
    "            print(f\"\\nLimite de páginas ({limite_pagina}) atingido. Parando a busca.\")\n",
    "            break\n",
    "\n",
    "        # Prepara os filtros para a página ATUAL\n",
    "        params_paginados = parametros.copy() # .copy() é vital para não zoar o original\n",
    "        params_paginados['pagina'] = pagina_atual \n",
    "        print(f\"\\n>>>> BUSCANDO PÁGINA: {pagina_atual} de {total_paginas_api}\")\n",
    "        \n",
    "        # 2. Chama o \"robô\" (definido na Célula 4)\n",
    "        retorno_fetch = fetch_com_retry(url_completa, params_paginados)\n",
    "        \n",
    "        # Se o robô falhar (retornar None), aborta tudo\n",
    "        if retorno_fetch is None:\n",
    "            print(\"Erro (fetch_com_retry retornou None). FIM DA BUSCA.\")\n",
    "            break \n",
    "            \n",
    "        # \"Desempacota\" a tupla (lista_de_dados, total_de_paginas)\n",
    "        dados_pagina, total_paginas_resposta = retorno_fetch\n",
    "        \n",
    "        # Atualiza o total de páginas (só na primeira vez, ou se mudar)\n",
    "        if total_paginas_resposta is not None:\n",
    "            total_paginas_api = total_paginas_resposta\n",
    "\n",
    "        # Condição de parada: Se a API retornar uma lista vazia, acabaram os dados.\n",
    "        if not dados_pagina: \n",
    "            print(f\"Fim dos dados (lista vazia) na página {pagina_atual}. FIM DA BUSCA.\")\n",
    "            break \n",
    "            \n",
    "        # 4. PERSISTÊNCIA: Chama o \"pedreiro\" (definido na Célula 4)\n",
    "        inserir_dados(nome_do_banco, dados_pagina, pagina_atual)\n",
    "        total_registros_processados += len(dados_pagina)\n",
    "\n",
    "        # 5. Incrementa o contador para a próxima página\n",
    "        pagina_atual += 1\n",
    "        # Pausa de meio segundo para ser \"educado\" com a API\n",
    "        time.sleep(0.5) \n",
    "        \n",
    "    print(\"\\n--- BUSCA CONCLUÍDA ---\")\n",
    "    print(f\"Total de registros processados: {total_registros_processados}\")\n",
    "    return True\n",
    "\n",
    "print(\"--- CÉLULA 5: Função 'buscar_dados_paginados' definida. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdbbf543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CÉLULA 6: Função 'preprocessar_texto' definida. ---\n"
     ]
    }
   ],
   "source": [
    "# CÉLULA 6: DEFINIÇÃO DA FERRAMENTA - preprocessar_texto\n",
    "# Digere os dados brutos através de Expressões Regulares e Tokenização\n",
    "\n",
    "def preprocessar_texto(texto):\n",
    "    \"\"\"Limpa e normaliza uma string de texto para o ML.\"\"\"\n",
    "    \n",
    "    # Etapa 1: Checagem de Segurança\n",
    "    # Se o 'objeto_compra' for Nulo ou um BLOB, ele não será 'str'.\n",
    "    # Retornamos uma string vazia para evitar erros.\n",
    "    if not isinstance(texto, str):\n",
    "        return \"\"\n",
    "\n",
    "    # Etapa 2: Normalização\n",
    "    # Converte \"AQUISIÇÃO\" para \"aquisição\"\n",
    "    texto_etapa2 = texto.lower()\n",
    "    \n",
    "    # Etapa 3: Limpeza de Ruído (Regex)\n",
    "    # [^a-zÀ-ú\\\\s] : \"encontre qualquer caractere que NÃO seja (^)\n",
    "    # uma letra de a-z, OU uma letra acentuada (À-ú), OU (\\\\) um espaço (s)\"\n",
    "    # e substitua por um espaço em branco ' '.\n",
    "    texto_etapa3 = re.sub(r'[^a-zÀ-ú\\s]', ' ', texto_etapa2, flags=re.IGNORECASE)\n",
    "    \n",
    "    # '\\\\s+' : \"encontre um ou mais (s+) espaços em sequência\"\n",
    "    # e substitua por um espaço único ' '.\n",
    "    texto_etapa3 = re.sub(r'\\\\s+', ' ', texto_etapa3).strip() # .strip() remove espaços no início/fim\n",
    "    \n",
    "    # Etapa 4: Tokenização (Virar uma lista de palavras)\n",
    "    # Encontra todas as \"palavras\" (sequências de letras)\n",
    "    tokens_etapa4 = re.findall(r'\\b[a-zÀ-ú]+\\b', texto_etapa3)\n",
    "    \n",
    "    # Etapa 5: Remoção de Stopwords\n",
    "    # List comprehension:\n",
    "    # \"Para cada 'palavra' na minha lista 'tokens_etapa4'...\"\n",
    "    tokens_etapa5 = [palavra for palavra in tokens_etapa4 \n",
    "                     # \"...mantenha-a SOMENTE SE...\"\n",
    "                     # \"...ela NÃO estiver na minha lista de stopwords (definida na Célula 2)...\"\n",
    "                     if palavra not in stop_words_pt \n",
    "                     # \"...E (and) o tamanho dela for maior que 2.\" (remove 'i', 'é', 'se')\n",
    "                     and len(palavra) > 2]\n",
    "    \n",
    "    # Etapa 6: Remontagem da String\n",
    "    # Junta a lista de palavras limpas de volta em uma string, separada por espaço.\n",
    "    # Ex: ['medidor', 'vazao'] -> \"medidor vazao\"\n",
    "    return \" \".join(tokens_etapa5)\n",
    "\n",
    "print(\"--- CÉLULA 6: Função 'preprocessar_texto' definida. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866aa975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CÉLULA 7: INICIANDO DEMONSTRAÇÃO DA COLETA ---\n",
      "\n",
      "[DEMO] Salvando dados em: 'pncp_data_DEMO_3pag.db'\n",
      "[DEMO] Parâmetros: {'dataFinal': 20251216, 'codigoModalidadeContratacao': 6, 'pagina': 1, 'tamanhoPagina': 50}\n",
      "[DEMO] Limite de páginas: 3\n",
      "\n",
      "Chamando a função 'buscar_dados_paginados' (definida na Célula 5)...\n",
      "Configurando banco de dados: pncp_data_DEMO_3pag.db\n",
      "Banco 'pncp_data_DEMO_3pag.db' e tabela 'contratacoes' configurados.\n",
      "--- INICIANDO BUSCA PAGINADA ---\n",
      "--- Modo LIMITE DE PÁGINAS Ativado: Parar após página 3 ---\n",
      "\n",
      ">>>> BUSCANDO PÁGINA: 1 de ???\n",
      "   -> [DB] 50/50 registros novos salvos na página 1.\n",
      "\n",
      ">>>> BUSCANDO PÁGINA: 2 de 400\n",
      "   -> [DB] 50/50 registros novos salvos na página 2.\n",
      "\n",
      ">>>> BUSCANDO PÁGINA: 3 de 400\n",
      "   -> [DB] 50/50 registros novos salvos na página 3.\n",
      "\n",
      "Limite de páginas (3) atingido. Parando a busca.\n",
      "\n",
      "--- BUSCA CONCLUÍDA ---\n",
      "Total de registros processados: 150\n",
      "\n",
      "--- DEMONSTRAÇÃO DA COLETA CONCLUÍDA ---\n",
      "Arquivo 'pncp_data_DEMO_3pag.db' foi criado no seu diretório.\n",
      "Ele contém aprox. 150 (ou menos) licitações do tipo 'Dispensa'.\n"
     ]
    }
   ],
   "source": [
    "# CÉLULA 7: [AÇÃO 1] - DEMONSTRAÇÃO DA COLETA DE DADOS\n",
    "# WTF: Rodando uma coleta PEQUENA (3 páginas) para provar que o \"Pescador\" funciona.\n",
    "#      Vamos salvar em um DB de \"demo\" separado para não zoar os arquivos de produção.\n",
    "\n",
    "print(\"--- CÉLULA 7: INICIANDO DEMONSTRAÇÃO DA COLETA ---\")\n",
    "\n",
    "# --- 1. Definir configurações LOCAIS para este teste ---\n",
    "\n",
    "# Nome do arquivo de banco de dados SÓ PARA ESTE TESTE\n",
    "NOME_DB_DEMO = \"pncp_data_DEMO_3pag.db\" \n",
    "\n",
    "# Limite de páginas para o teste (como solicitado: 3)\n",
    "LIMITE_PAG_DEMO = 3\n",
    "\n",
    "# Parâmetros customizados para o teste\n",
    "# Vamos pegar licitações de DISPENSA (cód 8) só para ser diferente\n",
    "params_demo = {\n",
    "    'dataFinal': 20251216,                \n",
    "    'codigoModalidadeContratacao': 6,\n",
    "    'pagina': 1,                          \n",
    "    'tamanhoPagina': 50                   \n",
    "}\n",
    "\n",
    "print(f\"\\n[DEMO] Salvando dados em: '{NOME_DB_DEMO}'\")\n",
    "print(f\"[DEMO] Parâmetros: {params_demo}\")\n",
    "print(f\"[DEMO] Limite de páginas: {LIMITE_PAG_DEMO}\")\n",
    "print(\"\\nChamando a função 'buscar_dados_paginados' (definida na Célula 5)...\")\n",
    "\n",
    "# --- 2. Chamar o \"Chefe\" (definido na Célula 5) com as configs de DEMO ---\n",
    "# Usamos as variáveis locais que acabamos de criar.\n",
    "try:\n",
    "    buscar_dados_paginados(\n",
    "        NOME_DB_DEMO, \n",
    "        URL_FINAL, \n",
    "        params_demo, \n",
    "        LIMITE_PAG_DEMO\n",
    "    )\n",
    "    print(\"\\n--- DEMONSTRAÇÃO DA COLETA CONCLUÍDA ---\")\n",
    "    print(f\"Arquivo '{NOME_DB_DEMO}' foi criado no seu diretório.\")\n",
    "    print(f\"Ele contém aprox. {LIMITE_PAG_DEMO * 50} (ou menos) licitações do tipo 'Pregão'.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nERRO durante a coleta de demonstração: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fe7dbcf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CÉLULA 8: INICIANDO ETL DE TREINAMENTO (Criação do Gabarito) ---\n",
      "Carregando 1s de: ['dataset_2024.csv', 'dataset_2025.csv']\n",
      "Total de 197 exemplos POSITIVOS (1) carregados dos CSVs.\n",
      "Carregando e rotulando dados do 'pncp_data_500.db'...\n",
      "Carregados 500 itens do DB de treino.\n",
      "Removidos BLOBs/Nulos. Restam 500 itens válidos.\n",
      "Rotulagem manual do DB de 500 concluída.\n",
      "Combinando todos os datasets...\n",
      "\n",
      "--- SUCESSO! (ETL de Treino) ---\n",
      "Arquivo final 'dataset.csv' criado com 672 exemplos.\n",
      "Balanceamento final do dataset (limpo e revisado):\n",
      "Relevante\n",
      "0    475\n",
      "1    197\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# CÉLULA 8: [AÇÃO 2] - RODAR O ETL DE TREINAMENTO\n",
    "# APERTE AQUI para criar o \"gabarito\" ('dataset.csv').\n",
    "# Junta os CSVs 2024 e 205 (1s) e o DB de 500 (Maioria 0s/1s) num arquivo só.\n",
    "\n",
    "print(\"--- CÉLULA 8: INICIANDO ETL DE TREINAMENTO (Criação do Gabarito) ---\")\n",
    "\n",
    "try:\n",
    "    # --- 1. Carregar dados POSITIVOS (1s) dos CSVs ---\n",
    "    # Pega o nome dos arquivos (ex: [\"dataset_2024.csv\", ...]) da Célula 2\n",
    "    print(f\"Carregando 1s de: {ARQUIVOS_CSV_POSITIVOS}\")\n",
    "    \n",
    "    # Cria uma lista vazia para guardar os DataFrames (tabelas)\n",
    "    lista_dfs_positivos = []\n",
    "    \n",
    "    # Loop nos nomes dos arquivos\n",
    "    for arquivo in ARQUIVOS_CSV_POSITIVOS:\n",
    "        \n",
    "        # Lê o CSV, mas 'usecols' pega SÓ as colunas 'x' e 'y' (ignora 'texto_bruto')\n",
    "        df_temp = pd.read_csv(arquivo, usecols=['x', 'y'], dtype={'y': str})\n",
    "        \n",
    "        # Adiciona a tabela lida à nossa lista\n",
    "        lista_dfs_positivos.append(df_temp)\n",
    "\n",
    "    # 'concat' empilha os DataFrames da lista (2024 + 2025) em um só\n",
    "    df_positivos = pd.concat(lista_dfs_positivos, ignore_index=True)\n",
    "    \n",
    "    # Renomeia as colunas 'x' -> 'Objeto' e 'y' -> 'Relevante'\n",
    "    df_positivos = df_positivos.rename(columns={'x': 'Objeto', 'y': 'Relevante'})\n",
    "    \n",
    "    # Garante que todos sejam '1', por segurança\n",
    "    df_positivos['Relevante'] = 1 \n",
    "    print(f\"Total de {len(df_positivos)} exemplos POSITIVOS (1) carregados dos CSVs.\")\n",
    "\n",
    "    # --- 2. Carregar e Rotular dados do DB de 500 (Negativos) ---\n",
    "    # Pega o nome do DB de 500 amostras da Célula 2\n",
    "    print(f\"Carregando e rotulando dados do '{NOME_DB_TREINO_NEGATIVOS}'...\")\n",
    "    conn = sqlite3.connect(NOME_DB_TREINO_NEGATIVOS)\n",
    "    \n",
    "    # Query para pegar o ID (para rotular) e o objeto_compra (o texto)\n",
    "    query_500 = \"SELECT id, objeto_compra FROM contratacoes\"\n",
    "    df_500 = pd.read_sql_query(query_500, conn) # Lê o DB para um DataFrame\n",
    "    conn.close()\n",
    "    print(f\"Carregados {len(df_500)} itens do DB de treino.\")\n",
    "\n",
    "    # [SOLUÇÃO BLOBs 1] Remove linhas onde 'objeto_compra' é Nulo (None)\n",
    "    df_500 = df_500.dropna(subset=['objeto_compra'])\n",
    "    \n",
    "    # [SOLUÇÃO BLOBs 2] Remove linhas onde o texto é literalmente a palavra \"BLOB\"\n",
    "    # O '~' significa NÃO (inverte a seleção)\n",
    "    df_500 = df_500[~df_500['objeto_compra'].str.contains('BLOB', na=False, case=False)]\n",
    "    print(f\"Removidos BLOBs/Nulos. Restam {len(df_500)} itens válidos.\")\n",
    "\n",
    "    # [SOLUÇÃO Labels Manuais] Define a função-dicionário (o \"caderno\")\n",
    "    def rotular_do_caderno(id):\n",
    "        # IDS_RELEVANTES_CADERNO foi definido na Célula 2\n",
    "        return 1 if id in IDS_RELEVANTES_CADERNO else 0\n",
    "            \n",
    "    # '.apply()' roda a função 'rotular_do_caderno' para cada 'id' no DataFrame\n",
    "    df_500['Relevante'] = df_500['id'].apply(rotular_do_caderno)\n",
    "    \n",
    "    # Renomeia a coluna de texto para bater com o df_positivos\n",
    "    df_500 = df_500.rename(columns={'objeto_compra': 'Objeto'})\n",
    "    print(\"Rotulagem manual do DB de 500 concluída.\")\n",
    "    \n",
    "    # --- 3. Combinar TUDO ---\n",
    "    print(\"Combinando todos os datasets...\")\n",
    "    \n",
    "    # Empilha o 'df_positivos' e o 'df_500' (só as colunas 'Objeto' e 'Relevante')\n",
    "    df_completo = pd.concat([df_positivos, df_500[['Objeto', 'Relevante']]], ignore_index=True)\n",
    "    \n",
    "    # Remove duplicatas de 'Objeto'.\n",
    "    # 'keep='first'' é crucial: se um objeto estava nos CSVs (1s) E no DB (0s),\n",
    "    # ele mantém o PRIMEIRO que viu (o '1' do CSV), garantindo a qualidade dos dados.\n",
    "    df_final = df_completo.drop_duplicates(subset=['Objeto'], keep='first')\n",
    "    \n",
    "    # --- 4. Salvar o \"Gabarito\" Final ---\n",
    "    # Salva o DataFrame final no 'dataset.csv' (nome definido na Célula 2)\n",
    "    df_final.to_csv(NOME_ARQUIVO_GABARITO, index=False, encoding='utf-8-sig')\n",
    "    \n",
    "    print(f\"\\n--- SUCESSO! (ETL de Treino) ---\")\n",
    "    print(f\"Arquivo final '{NOME_ARQUIVO_GABARITO}' criado com {len(df_final)} exemplos.\")\n",
    "    print(\"Balanceamento final do dataset (limpo e revisado):\")\n",
    "    \n",
    "    # Mostra quantos 0s e 1s temos no total\n",
    "    print(df_final['Relevante'].value_counts())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"ERRO CRÍTICO no ETL de Treino (Célula 8): {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81775ab6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CÉLULA 9: INICIANDO TREINAMENTO DO MODELO ---\n",
      "Lendo gabarito: 'dataset.csv'\n",
      "Salvando cérebro em: 'modelo_relevancia.joblib'\n",
      "\n",
      "Carregados 672 exemplos (licitações) do gabarito.\n",
      "Balanceamento real:\n",
      "Relevante\n",
      "0    475\n",
      "1    197\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Aplicando 'preprocessar_texto'...\n",
      "Dividindo em Treino (80%) e Teste (20%)...\n",
      "Pipeline definido com 'class_weight=balanced'.\n",
      "\n",
      "--- TREINANDO O MODELO (método .fit()) ---\n",
      "TREINAMENTO CONCLUÍDO.\n",
      "\n",
      "--- AVALIANDO O MODELO (nos 20% de teste) ---\n",
      "\n",
      "Relatório de Classificação Detalhado:\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "Irrelevante (0)       0.98      0.98      0.98        95\n",
      "  Relevante (1)       0.95      0.95      0.95        40\n",
      "\n",
      "       accuracy                           0.97       135\n",
      "      macro avg       0.96      0.96      0.96       135\n",
      "   weighted avg       0.97      0.97      0.97       135\n",
      "\n",
      "\n",
      "--- Salvando o modelo treinado em 'modelo_relevancia.joblib' ---\n",
      "Modelo salvo com sucesso!\n"
     ]
    }
   ],
   "source": [
    "# CÉLULA 9: [AÇÃO 3] - RODAR O TREINAMENTO\n",
    "# WTF: APERTE AQUI para \"estudar\" o 'dataset.csv' e salvar o \"cérebro\".\n",
    "#      (Célula 11 original, com o 'class_weight' de elite)\n",
    "\n",
    "print(\"--- CÉLULA 9: INICIANDO TREINAMENTO DO MODELO ---\")\n",
    "# Pega os nomes de arquivos da Célula 2\n",
    "print(f\"Lendo gabarito: '{NOME_ARQUIVO_GABARITO}'\")\n",
    "print(f\"Salvando cérebro em: '{NOME_ARQUIVO_MODELO}'\")\n",
    "\n",
    "try:\n",
    "    # --- 1. Carregar o Gabarito ---\n",
    "    # Lê o arquivo 'dataset.csv' que a Célula 8 acabou de criar\n",
    "    df_treino = pd.read_csv(NOME_ARQUIVO_GABARITO)\n",
    "    print(f\"\\nCarregados {len(df_treino)} exemplos (licitações) do gabarito.\")\n",
    "    print(\"Balanceamento real:\")\n",
    "    print(df_treino['Relevante'].value_counts())\n",
    "    \n",
    "    # --- 2. Pré-processamento (Limpeza) ---\n",
    "    print(\"\\nAplicando 'preprocessar_texto'...\")\n",
    "    # Chama a função \"liquidificador\" (definida na Célula 6) para cada linha da coluna 'Objeto'\n",
    "    df_treino['objeto_limpo'] = df_treino['Objeto'].apply(preprocessar_texto)\n",
    "    \n",
    "    # --- 3. Separar X (Features) e y (Labels) ---\n",
    "    # X = O que o modelo lê (o texto limpo)\n",
    "    X = df_treino['objeto_limpo']\n",
    "    # y = A resposta correta (o gabarito 0 ou 1)\n",
    "    y = df_treino['Relevante']\n",
    "    \n",
    "    # --- 4. Dividir em Treino/Teste ---\n",
    "    print(\"Dividindo em Treino (80%) e Teste (20%)...\")\n",
    "    # 'test_size=0.2' = 20% para \"prova\" (teste)\n",
    "    # 'random_state=42' = garante que a divisão seja sempre a mesma (reprodutibilidade)\n",
    "    # 'stratify=y' = Garante que os 20% de teste tenham a *mesma proporção* de 0s e 1s\n",
    "    #                  que o dataset original. Muito importante!\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    \n",
    "    # --- 5. Definir o Pipeline (A \"Linha de Montagem\" de Elite) ---\n",
    "    modelo_pipeline = Pipeline([\n",
    "        # Passo 1: O \"Tradutor\" (TF-IDF)\n",
    "        ('tfidf', TfidfVectorizer(max_features=1000,    # Considera só as 1000 palavras mais importantes\n",
    "                                  ngram_range=(1, 2))), # Olha \"vazão\" (1) e \"medidor vazão\" (2)\n",
    "        \n",
    "        # Passo 2: O \"Aluno\" (Classificador)\n",
    "        ('clf', SGDClassifier(loss='hinge',              # 'hinge' é o \"motor\" (SVM), ótimo para isso\n",
    "                             random_state=42,          # Para reprodutibilidade\n",
    "                             max_iter=100,             # Número de \"passadas\" de estudo\n",
    "                             class_weight='balanced')) # A MÁGICA: \"Não seja preguiçoso!\"\n",
    "    ])\n",
    "    print(\"Pipeline definido com 'class_weight=balanced'.\")\n",
    "\n",
    "    # --- 6. TREINAR ---\n",
    "    print(\"\\n--- TREINANDO O MODELO (método .fit()) ---\")\n",
    "    # Manda a \"Linha de Montagem\" (Pipeline) \"estudar\" os dados de treino\n",
    "    modelo_pipeline.fit(X_train, y_train)\n",
    "    print(\"TREINAMENTO CONCLUÍDO.\")\n",
    "\n",
    "    # --- 7. AVALIAR ---\n",
    "    print(\"\\n--- AVALIANDO O MODELO (nos 20% de teste) ---\")\n",
    "    # Manda o modelo treinado \"fazer a prova\" (prever os 20% que ele nunca viu)\n",
    "    y_pred = modelo_pipeline.predict(X_test)\n",
    "    \n",
    "    print(\"\\nRelatório de Classificação Detalhado:\")\n",
    "    # Imprime o \"Boletim\": compara as previsões (y_pred) com o gabarito (y_test)\n",
    "    print(classification_report(y_test, y_pred, target_names=['Irrelevante (0)', 'Relevante (1)']))\n",
    "\n",
    "    # --- 8. SALVAR O CÉREBRO ---\n",
    "    print(f\"\\n--- Salvando o modelo treinado em '{NOME_ARQUIVO_MODELO}' ---\")\n",
    "    # \"Congela\" o modelo treinado e salva em um arquivo\n",
    "    joblib.dump(modelo_pipeline, NOME_ARQUIVO_MODELO)\n",
    "    print(\"Modelo salvo com sucesso!\")\n",
    "\n",
    "# Se o 'dataset.csv' não existir...\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERRO CRÍTICO: Arquivo '{NOME_ARQUIVO_GABARITO}' não encontrado.\")\n",
    "    print(\"Rode a CÉLULA 8 (ETL de Treino) primeiro.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro inesperado no Treino: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa219514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CÉLULA 10: INICIANDO APLICAÇÃO EM PRODUÇÃO (FILTRO) ---\n",
      "Carregando cérebro: 'modelo_relevancia.joblib'\n",
      "Lendo DB de Produção: 'pncp_data_19000.db'\n",
      "Salvando filé em: 'licitacoes_filtradas_19k.db'\n",
      "\n",
      "Carregando modelo 'modelo_relevancia.joblib'...\n",
      "Modelo carregado com sucesso.\n",
      "Conectando ao SQLite 'pncp_data_19000.db' (em lotes de 2000)...\n",
      "\n",
      "--- Processando lote... (Itens até 2000) ---\n",
      "==> 78 itens RELEVANTES encontrados neste lote.\n",
      "\n",
      "--- Processando lote... (Itens até 4000) ---\n",
      "==> 92 itens RELEVANTES encontrados neste lote.\n",
      "\n",
      "--- Processando lote... (Itens até 6000) ---\n",
      "==> 108 itens RELEVANTES encontrados neste lote.\n",
      "\n",
      "--- Processando lote... (Itens até 8000) ---\n",
      "==> 97 itens RELEVANTES encontrados neste lote.\n",
      "\n",
      "--- Processando lote... (Itens até 10000) ---\n",
      "==> 101 itens RELEVANTES encontrados neste lote.\n",
      "\n",
      "--- Processando lote... (Itens até 12000) ---\n",
      "==> 110 itens RELEVANTES encontrados neste lote.\n",
      "\n",
      "--- Processando lote... (Itens até 14000) ---\n",
      "==> 87 itens RELEVANTES encontrados neste lote.\n",
      "\n",
      "--- Processando lote... (Itens até 16000) ---\n",
      "==> 93 itens RELEVANTES encontrados neste lote.\n",
      "\n",
      "--- Processando lote... (Itens até 18000) ---\n",
      "==> 108 itens RELEVANTES encontrados neste lote.\n",
      "\n",
      "--- Processando lote... (Itens até 19947) ---\n",
      "==> 91 itens RELEVANTES encontrados neste lote.\n",
      "\n",
      "--- Todos os lotes foram processados. ---\n",
      "\n",
      "--- RESULTADO DA FILTRAGEM TOTAL ---\n",
      "De 19947 itens totais, 965 foram classificados como RELEVANTES.\n",
      "\n",
      "--- Salvando os dados filtrados em 'licitacoes_filtradas_19k.db' ---\n",
      "\n",
      "Arquivo 'licitacoes_filtradas_19k.db' salvo com sucesso!\n",
      "Amostra do filé (primeiras 10 linhas):\n",
      "                        pncp_id  \\\n",
      "0  37115383000153-1-000051/2025   \n",
      "1  07954605000160-1-000177/2025   \n",
      "2  07954480000179-1-024862/2024   \n",
      "3  07954480000179-1-012834/2025   \n",
      "4  42498733000148-1-001778/2025   \n",
      "5  07954480000179-1-017042/2025   \n",
      "6  07954480000179-1-017915/2025   \n",
      "7  00394544000185-1-002205/2025   \n",
      "8  17947581000176-1-000180/2025   \n",
      "9  18291351000164-1-000229/2025   \n",
      "\n",
      "                                       objeto_compra  \n",
      "0  Contratação de empresa especializada para pres...  \n",
      "1  Constitui objeto da presente licitação o regis...  \n",
      "2  O objeto da licitação é a prestação de serviço...  \n",
      "3  O objeto da licitação é a aquisição de EQUIPAM...  \n",
      "4  Policloreto de alumínio visando atender às nec...  \n",
      "5  O objeto da licitação é o Registro de Preço pa...  \n",
      "6  O objeto da licitação é a aquisição de equipam...  \n",
      "7  Aquisição de materiais e equipamentos para Sis...  \n",
      "8  Aquisição de materiais e insumos odontológicos...  \n",
      "9  Contratação de empresa para fornecimento e ins...  \n",
      "\n",
      "--- CÉLULA 10 (Produção) concluída ---\n"
     ]
    }
   ],
   "source": [
    "# CÉLULA 10: [AÇÃO 4] - RODAR A PRODUÇÃO (O FILTRO)\n",
    "# WTF: APERTE AQUI para usar o \"cérebro\" e filtrar seu DB de 19.9k.\n",
    "#      (Célula 12 original, com o processamento em lotes)\n",
    "\n",
    "print(\"--- CÉLULA 10: INICIANDO APLICAÇÃO EM PRODUÇÃO (FILTRO) ---\")\n",
    "# Pega os nomes de arquivos da Célula 2\n",
    "print(f\"Carregando modelo treinado: '{NOME_ARQUIVO_MODELO}'\")\n",
    "print(f\"Lendo DB de Produção: '{NOME_DB_PRODUCAO}'\")\n",
    "print(f\"Salvando licitações filtradas em: '{NOME_DB_FILTRADO}'\")\n",
    "\n",
    "try:\n",
    "    # --- 1. Carregar o Cérebro ---\n",
    "    print(f\"\\nCarregando modelo '{NOME_ARQUIVO_MODELO}'...\")\n",
    "    # Carrega o 'modelo_relevancia.joblib' que a Célula 9 criou\n",
    "    modelo_carregado = joblib.load(NOME_ARQUIVO_MODELO)\n",
    "    print(\"Modelo carregado com sucesso.\")\n",
    "\n",
    "    # --- 2. Ler DB de Produção (em lotes) ---\n",
    "    print(f\"Conectando ao SQLite '{NOME_DB_PRODUCAO}' (em lotes de {TAMANHO_DO_LOTE})...\")\n",
    "    conn = sqlite3.connect(NOME_DB_PRODUCAO)\n",
    "    \n",
    "    # Pega todas as colunas que queremos no resultado final\n",
    "    query_sql = \"SELECT id, pncp_id, orgao_nome, uf_sigla, objeto_compra, dados_json FROM contratacoes\"\n",
    "    \n",
    "    # Lista vazia para guardar as licitações filtradas de cada lote\n",
    "    lista_de_resultados = []\n",
    "    total_processado = 0\n",
    "    total_relevante = 0\n",
    "\n",
    "    # [Prevenir o Limite de uso Da Memória RAM]\n",
    "    # 'pd.read_sql_query' com 'chunksize' não lê o DB todo.\n",
    "    # Ele cria um \"iterador\" que lê o DB em pedaços (lotes - chunks) de 2000 linhas.\n",
    "    for chunk_df in pd.read_sql_query(query_sql, conn, chunksize=TAMANHO_DO_LOTE):\n",
    "        \n",
    "        total_processado += len(chunk_df)\n",
    "        print(f\"\\n--- Processando lote... (Itens até {total_processado}) ---\")\n",
    "        \n",
    "        # 3a. Limpeza do Lote (filtro de BLOBs)\n",
    "        chunk_limpo = chunk_df.dropna(subset=['objeto_compra']).copy()\n",
    "        chunk_limpo = chunk_limpo[~chunk_limpo['objeto_compra'].str.contains('BLOB', na=False, case=False)]\n",
    "        \n",
    "        if len(chunk_limpo) == 0:\n",
    "            print(\"Lote vazio após limpeza. Pulando...\")\n",
    "            continue\n",
    "            \n",
    "        # 3b. Predição do Lote\n",
    "        # Aplica o preprocessamento de teto (Célula 6) no lote\n",
    "        chunk_limpo['objeto_limpo'] = chunk_limpo['objeto_compra'].apply(preprocessar_texto)\n",
    "        \n",
    "        # Usa o modelo treinado (carregado) para classificar o lote\n",
    "        predicoes = modelo_carregado.predict(chunk_limpo['objeto_limpo'])\n",
    "        chunk_limpo['relevante_pred'] = predicoes\n",
    "        \n",
    "        # 3c. Filtragem do Lote\n",
    "        # Pega só as linhas que o modelo disse que são '1'\n",
    "        df_relevante_do_lote = chunk_limpo[chunk_limpo['relevante_pred'] == 1]\n",
    "        \n",
    "        if len(df_relevante_do_lote) > 0:\n",
    "            print(f\"==> {len(df_relevante_do_lote)} itens RELEVANTES encontrados neste lote.\")\n",
    "            # Adiciona as licitações filtradas do lote na nossa lista de resultados\n",
    "            lista_de_resultados.append(df_relevante_do_lote)\n",
    "            total_relevante += len(df_relevante_do_lote)\n",
    "        else:\n",
    "            print(\"Nenhum item relevante neste lote.\")\n",
    "            \n",
    "    conn.close()\n",
    "    print(\"\\n--- Todos os lotes foram processados. ---\")\n",
    "\n",
    "    # --- 4. SALVAMENTO DO RESULTADO FINAL ---\n",
    "    if not lista_de_resultados:\n",
    "        print(\"ALERTA: Nenhum item relevante foi encontrado no banco de dados inteiro.\")\n",
    "    \n",
    "    else:\n",
    "        # 'concat' junta as licitações filtradas de todos os lotes em um DataFrame final\n",
    "        df_relevante_final = pd.concat(lista_de_resultados, ignore_index=True)\n",
    "        \n",
    "        print(f\"\\n--- RESULTADO DA FILTRAGEM TOTAL ---\")\n",
    "        print(f\"De {total_processado} itens totais, {total_relevante} foram classificados como RELEVANTES.\")\n",
    "\n",
    "        print(f\"\\n--- Salvando os dados filtrados em '{NOME_DB_FILTRADO}' ---\")\n",
    "        # Define as colunas que queremos no DB final (excluindo 'objeto_limpo', etc.)\n",
    "        colunas_finais = ['id', 'pncp_id', 'orgao_nome', 'uf_sigla', 'objeto_compra', 'dados_json']\n",
    "        df_para_salvar = df_relevante_final[colunas_finais]\n",
    "        \n",
    "        # Cria um novo DB de saída\n",
    "        conn_out = sqlite3.connect(NOME_DB_FILTRADO)\n",
    "        # Salva o DataFrame final na tabela 'licitacoes_filtradas'\n",
    "        df_para_salvar.to_sql('licitacoes_filtradas', conn_out, if_exists='replace', index=False)\n",
    "        conn_out.close()\n",
    "        \n",
    "        print(f\"\\nArquivo '{NOME_DB_FILTRADO}' salvo com sucesso!\")\n",
    "        print(\"Amostra das licitações filtradas (primeiras 10 linhas):\")\n",
    "        print(df_para_salvar[['pncp_id', 'objeto_compra']].head(10))\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERRO CRÍTICO: Modelo '{NOME_ARQUIVO_MODELO}' ou DB '{NOME_DB_PRODUCAO}' não encontrado.\")\n",
    "    print(\"Rode a CÉLULA 9 (Treinamento) primeiro.\")\n",
    "except Exception as e:\n",
    "    print(f\"Ocorreu um erro inesperado: {e}\")\n",
    "    print(f\"Verifique se a CÉLULA 6 ('preprocessar_texto') foi rodada.\")\n",
    "\n",
    "print(\"\\n--- CÉLULA 10 (Produção) concluída ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
